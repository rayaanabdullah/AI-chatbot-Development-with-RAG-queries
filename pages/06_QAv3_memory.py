import streamlit as st
from st_supabase_connection import SupabaseConnection
from langchain_community.vectorstores.supabase import SupabaseVectorStore
from langchain_openai.embeddings import OpenAIEmbeddings
from langchain_openai.chat_models import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains.combine_documents.stuff import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
import time

# Load secrets
SUPABASE_URL = st.secrets["SUPABASE_URL"]
SUPABASE_KEY = st.secrets["SUPABASE_KEY"]
OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"]

st.set_page_config(page_title="ржмрж╛ржВрж▓рж╛ HSC26 RAG рж╕рж╣рж╛ржпрж╝ржХ", page_icon="ЁЯУЪ")
st.title("ЁЯУЪ ржмрж╛ржВрж▓рж╛ HSC26 RAG рж╕рж╣рж╛ржпрж╝ржХ")

# Initialize chat history
if "messages" not in st.session_state:
    st.session_state.messages = []

if "rag_chain" not in st.session_state:
    st.session_state.rag_chain = None

if "debug_mode" not in st.session_state:
    st.session_state.debug_mode = False

# Connection checks
@st.cache_resource
def check_connections():
    def check_supabase():
        try:
            sup = st.connection("supabase", type=SupabaseConnection,
                                url=SUPABASE_URL, key=SUPABASE_KEY)
            _ = sup.client.auth.get_user()
            return True, "тЬЕ Supabase Connected"
        except Exception as e:
            return False, f"тЭМ Supabase Error: {e}"

    def check_openai():
        try:
            from openai import OpenAI
            client = OpenAI(api_key=OPENAI_API_KEY)
            client.models.list()
            return True, "тЬЕ OpenAI Connected"
        except Exception as e:
            return False, f"тЭМ OpenAI Error: {e}"
    
    return check_supabase(), check_openai()

# Setup RAG chain
@st.cache_resource
def setup_rag_chain():
    try:
        supabase_conn = st.connection("supabase", type=SupabaseConnection,
                                      url=SUPABASE_URL, key=SUPABASE_KEY)
        
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small", openai_api_key=OPENAI_API_KEY)
        vectorstore = SupabaseVectorStore(client=supabase_conn.client, embedding=embeddings,
                                          table_name="documents", query_name="match_documents")
        retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
        
        # Enhanced system prompt with better context handling
        system_prompt = """рждрзБржорж┐ ржПржХржЬржи рж╕рж╣рж╛ржпрж╝ржХ ржмрж╛ржВрж▓рж╛ рж╕рж╣ржХрж╛рж░рзА ржпрж┐ржирж┐ ржХржерзЛржкржХржержирзЗрж░ ржзрж╛рж░рж╛ржмрж╛рж╣рж┐ржХрждрж╛ ржмржЬрж╛ржпрж╝ рж░рж╛ржЦрзЗржиред

ржирж┐ржпрж╝ржорж╛ржмрж▓рзА:
1. ржирж┐ржЪрзЗрж░ рждржерзНржпрж╕рзВрждрзНрж░ ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзЗ ржЙрждрзНрждрж░ ржжрж╛ржУ
2. ржпржжрж┐ рждржерзНржпрж╕рзВрждрзНрж░рзЗ рж╕рж░рж╛рж╕рж░рж┐ ржЙрждрзНрждрж░ ржирж╛ ржерж╛ржХрзЗ, рждржмрзЗ ржмрж▓рзЛ 'рждржерзНржп ржирзЗржЗ'
3. ржкрзВрж░рзНржмржмрж░рзНрждрзА ржХржерзЛржкржХржержирзЗрж░ ржкрзНрж░рж╕ржЩрзНржЧ ржоржирзЗ рж░рзЗржЦрзЛ
4. ржмрзНржпржмрж╣рж╛рж░ржХрж╛рж░рзА ржпржжрж┐ ржкрзВрж░рзНржмрзЗрж░ ржкрзНрж░рж╕ржЩрзНржЧ ржЙрж▓рзНрж▓рзЗржЦ ржХрж░рзЗ (ржпрзЗржоржи "рж╕рзЗ", "рждрж╛рж░", "ржПржЯрж╛", "ржР ржмрзНржпржХрзНрждрж┐"), рждрж╛рж╣рж▓рзЗ ржХржерзЛржкржХржержирзЗрж░ ржЗрждрж┐рж╣рж╛рж╕ ржЕржирзБржпрж╛ржпрж╝рзА ржмрзБржЭрзЗ ржирж╛ржУ
5. ржмрзНржпржмрж╣рж╛рж░ржХрж╛рж░рзА ржпржжрж┐ ржЖржЧрзЗрж░ ржкрзНрж░рж╢рзНржи ржмрж╛ ржЙрждрзНрждрж░ рж╕ржорзНржкрж░рзНржХрзЗ ржЬрж┐ржЬрзНржЮрж╛рж╕рж╛ ржХрж░рзЗ, рждрж╛рж╣рж▓рзЗ ржХржерзЛржкржХржержирзЗрж░ ржЗрждрж┐рж╣рж╛рж╕ ржерзЗржХрзЗ ржЙрждрзНрждрж░ ржжрж╛ржУ

ржкрзВрж░рзНржмржмрж░рзНрждрзА ржХржерзЛржкржХржержи:
{conversation_history}

ржмрж░рзНрждржорж╛ржи рждржерзНржпрж╕рзВрждрзНрж░:
{context}

ржЧрзБрж░рзБрждрзНржмржкрзВрж░рзНржг: ржмрзНржпржмрж╣рж╛рж░ржХрж╛рж░рзАрж░ ржкрзНрж░рж╢рзНржирзЗ ржпржжрж┐ ржЖржЧрзЗрж░ ржХржерзЛржкржХржержирзЗрж░ рж░рзЗржлрж╛рж░рзЗржирзНрж╕ ржерж╛ржХрзЗ, рж╕рзЗржЯрж╛ ржмрж┐ржмрзЗржЪржирж╛ ржХрж░рзЗ ржЙрждрзНрждрж░ ржжрж╛ржУред"""
        
        prompt = ChatPromptTemplate.from_messages([("system", system_prompt), ("human", "{input}")])
        llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.3, api_key=OPENAI_API_KEY)
        qa_chain = create_stuff_documents_chain(llm, prompt)
        rag_chain = create_retrieval_chain(retriever, qa_chain)
        
        return rag_chain, None
    except Exception as e:
        return None, str(e)

# Function to get detailed conversation context
def get_conversation_context():
    if len(st.session_state.messages) == 0:
        return ""
    
    # Get last 6 messages for better context (3 exchanges)
    recent_messages = st.session_state.messages[-6:]
    context = ""
    
    for i, msg in enumerate(recent_messages):
        role = "ржмрзНржпржмрж╣рж╛рж░ржХрж╛рж░рзА" if msg["role"] == "user" else "рж╕рж╣рж╛ржпрж╝ржХ"
        context += f"{role}: {msg['content']}\n"
    
    return context

# Function to check if question refers to previous context
def needs_conversation_context(question):
    """Check if the question refers to previous conversation"""
    context_indicators = [
        'рж╕рзЗ', 'рждрж┐ржирж┐', 'рждрж╛рж░', 'ржУрж░', 'ржПржЯрж╛', 'ржУржЯрж╛', 'ржПржЗ', 'ржР', 
        'ржЖржЧрзЗрж░', 'ржкрзВрж░рзНржмрзЗрж░', 'ржЖржорж┐ ржХрж┐', 'ржЖржорж┐ ржпрж╛', 'ржХрж┐ ржЬрж┐ржЬрзНржЮрзЗрж╕',
        'valo', 'ржнрж╛рж▓рзЛ', 'she', 'рждрж╛рж╣рж▓рзЗ', 'why', 'ржХрзЗржи',
        'question', 'ржкрзНрж░рж╢рзНржи', 'ami ki', 'what i', 'so far'
    ]
    
    question_lower = question.lower()
    return any(indicator in question_lower for indicator in context_indicators)

# Check connections
(sup_status, sup_msg), (open_status, open_msg) = check_connections()

# Sidebar
with st.sidebar:
    st.markdown("### ЁЯФЧ Connection Status")
    st.write(sup_msg)
    st.write(open_msg)
    
    st.markdown("### ЁЯТм Chat Controls")
    st.write(f"ЁЯУЭ Messages: {len(st.session_state.messages)}")
    
    # Debug mode toggle
    st.session_state.debug_mode = st.checkbox("ЁЯРЫ Debug Mode", value=st.session_state.debug_mode)
    
    if st.button("ЁЯЧСя╕П Clear Chat History"):
        st.session_state.messages = []
        st.success("Chat cleared!")
        time.sleep(0.5)
        st.rerun()
    
    # Export chat option
    if len(st.session_state.messages) > 0:
        chat_export = ""
        for msg in st.session_state.messages:
            role = "ЁЯСд ржЖржкржирж┐" if msg["role"] == "user" else "ЁЯдЦ рж╕рж╣рж╛ржпрж╝ржХ"
            chat_export += f"{role}: {msg['content']}\n\n"
        
        st.download_button(
            "ЁЯТ╛ Export Chat",
            data=chat_export,
            file_name=f"chat_history_{int(time.time())}.txt",
            mime="text/plain"
        )

# Debug section
if st.session_state.debug_mode:
    with st.sidebar:
        st.markdown("### ЁЯФН Debug Info")
        if len(st.session_state.messages) > 0:
            st.write("**Last 3 Messages:**")
            for i, msg in enumerate(st.session_state.messages[-3:]):
                st.text(f"{i+1}. {msg['role']}: {msg['content'][:50]}...")
            
            st.write("**Current Context:**")
            context = get_conversation_context()
            st.text_area("Context", context, height=100, key="debug_context")

# Check if connections are working
if not (sup_status and open_status):
    st.error("тЭМ Connection failed. Please check your configuration.")
    st.stop()

# Setup RAG chain if not already done
if st.session_state.rag_chain is None:
    with st.spinner("Setting up RAG system..."):
        rag_chain, error = setup_rag_chain()
        if rag_chain:
            st.session_state.rag_chain = rag_chain
            st.success("тЬЕ RAG system ready!")
        else:
            st.error(f"тЭМ RAG setup failed: {error}")
            st.stop()

# Display chat messages
st.markdown("### ЁЯТм ржХржерзЛржкржХржержи")

# Create a container for chat messages
chat_container = st.container()

with chat_container:
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            # Show sources for assistant messages
            if message["role"] == "assistant" and "sources" in message:
                with st.expander("ЁЯУЪ рждржерзНржпрж╕рзВрждрзНрж░"):
                    for i, source in enumerate(message["sources"], 1):
                        st.markdown(f"**Source {i}:** {source}")

# Chat input
if prompt := st.chat_input("ржЖржкржирж╛рж░ ржкрзНрж░рж╢рзНржи рж▓рж┐ржЦрзБржи..."):
    # Add user message to chat history
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # Display user message
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # Generate assistant response
    with st.chat_message("assistant"):
        with st.spinner("ржЪрж┐ржирзНрждрж╛ ржХрж░ржЫрж┐..."):
            try:
                # Get conversation context
                conversation_context = get_conversation_context()
                
                # Check if question needs conversation context
                context_needed = needs_conversation_context(prompt)
                
                # Debug info
                if st.session_state.debug_mode:
                    st.write("ЁЯФН **Debug Info:**")
                    st.write(f"- Context needed: {context_needed}")
                    st.write(f"- Conversation length: {len(st.session_state.messages)}")
                    if conversation_context:
                        st.write(f"- Context preview: {conversation_context[:100]}...")
                
                # Create the input for RAG
                if context_needed and conversation_context:
                    # For context-dependent questions, add more conversation history
                    rag_input = {
                        "input": prompt,
                        "conversation_history": conversation_context
                    }
                else:
                    # For independent questions, minimal context
                    rag_input = {
                        "input": prompt,
                        "conversation_history": conversation_context[-200:] if conversation_context else ""
                    }
                
                # Get response from RAG
                result = st.session_state.rag_chain.invoke(rag_input)
                response = result.get("answer", "рждржерзНржп ржирзЗржЗ")
                
                # Extract sources
                sources = []
                if "context" in result and result["context"]:
                    sources = [doc.page_content[:200] + "..." for doc in result["context"]]
                
                # If asking about previous questions and no good answer, try to answer from chat history
                if (response == "рждржерзНржп ржирзЗржЗ" and context_needed and 
                    any(word in prompt.lower() for word in ['ami ki', 'what i', 'question', 'ржкрзНрж░рж╢рзНржи', 'ржЬрж┐ржЬрзНржЮрзЗрж╕'])):
                    
                    # Extract questions from chat history
                    user_questions = [msg["content"] for msg in st.session_state.messages if msg["role"] == "user"]
                    if user_questions:
                        if "so far" in prompt.lower() or "ржХрж┐ ржХрж┐" in prompt:
                            response = f"ржЖржкржирж┐ ржП ржкрж░рзНржпржирзНржд ржпрзЗ ржкрзНрж░рж╢рзНржиржЧрзБрж▓рзЛ ржХрж░рзЗржЫрзЗржи:\n\n"
                            for i, q in enumerate(user_questions[:-1], 1):  # Exclude current question
                                response += f"{i}. {q}\n"
                        else:
                            response = f"ржЖржкржирж╛рж░ рж╢рзЗрж╖ ржкрзНрж░рж╢рзНржи ржЫрж┐рж▓: '{user_questions[-2] if len(user_questions) > 1 else 'ржХрзЛржирзЛ ржЖржЧрзЗрж░ ржкрзНрж░рж╢рзНржи ржирзЗржЗ'}'"
                
                # Display response
                st.markdown(response)
                
                # Debug: Show what was sent to RAG
                if st.session_state.debug_mode:
                    st.write("ЁЯФз **RAG Input:**")
                    st.json(rag_input)
                
                # Add assistant response to chat history
                st.session_state.messages.append({
                    "role": "assistant", 
                    "content": response,
                    "sources": sources
                })
                
            except Exception as e:
                error_msg = f"ржжрзБржГржЦрж┐ржд, ржПржХржЯрж┐ рж╕ржорж╕рзНржпрж╛ рж╣ржпрж╝рзЗржЫрзЗ: {str(e)}"
                st.error(error_msg)
                st.session_state.messages.append({
                    "role": "assistant", 
                    "content": error_msg
                })

# Quick action buttons
if len(st.session_state.messages) == 0:
    st.markdown("### ЁЯЪА ржжрзНрж░рзБржд рж╢рзБрж░рзБ ржХрж░рзБржи")
    st.markdown("ржирж┐ржЪрзЗрж░ ржмрж┐рж╖ржпрж╝ржЧрзБрж▓рзЛ рж╕ржорзНржкрж░рзНржХрзЗ ржЬрж┐ржЬрзНржЮрж╛рж╕рж╛ ржХрж░рждрзЗ ржкрж╛рж░рзЗржи:")
    
    col1, col2, col3, col4 = st.columns(4)
    
    quick_questions = [
        ("ЁЯУЦ", "ржмрж╛ржВрж▓рж╛ рж╕рж╛рж╣рж┐рждрзНржп", "ржХрж▓рзНржпрж╛ржгрзАрж░ ржкрж┐рждрж╛рж░ ржирж╛ржо ржХрзА?")
    ]
    
    cols = [col1, col2, col3, col4]
    for i, (emoji, title, question) in enumerate(quick_questions):
        with cols[i]:
            if st.button(f"{emoji} {title}"):
                # Add the question as if user typed it
                st.session_state.messages.append({"role": "user", "content": question})
                st.rerun()

# Tips section
if len(st.session_state.messages) > 0:
    st.markdown("""
<div style='
    background-color: #23272f;
    padding: 14px 16px;
    border-radius: 8px;
    margin-top: 12px;
    color: #f3f6fb;
    font-size: 15px;
    border-left: 4px solid #3b82f6;'
>
    <small>
    ЁЯТб <strong>ржЯрж┐ржкрж╕:</strong><br>
    тАв "рж╕рзЗ ржХрзЗржоржи?" - ржЖржЧрзЗрж░ ржХржерзЛржкржХржержирзЗрж░ ржмрзНржпржХрзНрждрж┐ рж╕ржорзНржкрж░рзНржХрзЗ ржЬрж┐ржЬрзНржЮрзЗрж╕ ржХрж░рзБржи<br>
    тАв "ржЖржорж┐ ржХрж┐ ржХрж┐ ржкрзНрж░рж╢рзНржи ржХрж░рзЗржЫрж┐?" - ржЖржкржирж╛рж░ ржкрзНрж░рж╢рзНржирзЗрж░ рждрж╛рж▓рж┐ржХрж╛ ржжрзЗржЦрзБржи<br>
    тАв "ржПржЯрж╛ ржХрзА?" - ржЖржЧрзЗрж░ ржЙрж▓рзНрж▓рзЗржЦрж┐ржд ржмрж┐рж╖ржпрж╝ рж╕ржорзНржкрж░рзНржХрзЗ ржЖрж░ржУ ржЬрж╛ржирзБржи
    </small>
</div>
""", unsafe_allow_html=True)


# Footer
st.markdown("---")
st.markdown("""
<div style='text-align: center; color: #666; font-size: 12px;'>
    ЁЯЪА Powered by LangChain, Supabase, OpenAI, and Streamlit<br>
    ЁЯза Enhanced with Conversation Memory & Context Awareness
</div>
""", unsafe_allow_html=True)
