import streamlit as st
from st_supabase_connection import SupabaseConnection
from langchain_community.vectorstores.supabase import SupabaseVectorStore
from langchain_openai.embeddings import OpenAIEmbeddings
from langchain_openai.chat_models import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains.combine_documents.stuff import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
import time

# Load secrets
SUPABASE_URL = st.secrets["SUPABASE_URL"]
SUPABASE_KEY = st.secrets["SUPABASE_KEY"]
OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"]

st.set_page_config(page_title="‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ HSC26 RAG ‡¶∏‡¶π‡¶æ‡¶Ø‡¶º‡¶ï", page_icon="üìö")
st.title("üìö ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ HSC26 RAG ‡¶∏‡¶π‡¶æ‡¶Ø‡¶º‡¶ï")

# Initialize chat history
if "messages" not in st.session_state:
    st.session_state.messages = []

if "rag_chain" not in st.session_state:
    st.session_state.rag_chain = None

if "debug_mode" not in st.session_state:
    st.session_state.debug_mode = False

# Connection checks
@st.cache_resource
def check_connections():
    def check_supabase():
        try:
            sup = st.connection("supabase", type=SupabaseConnection,
                                url=SUPABASE_URL, key=SUPABASE_KEY)
            _ = sup.client.auth.get_user()
            return True, "‚úÖ Supabase Connected"
        except Exception as e:
            return False, f"‚ùå Supabase Error: {e}"

    def check_openai():
        try:
            from openai import OpenAI
            client = OpenAI(api_key=OPENAI_API_KEY)
            client.models.list()
            return True, "‚úÖ OpenAI Connected"
        except Exception as e:
            return False, f"‚ùå OpenAI Error: {e}"
    
    return check_supabase(), check_openai()

# Setup RAG chain
@st.cache_resource
def setup_rag_chain():
    try:
        supabase_conn = st.connection("supabase", type=SupabaseConnection,
                                      url=SUPABASE_URL, key=SUPABASE_KEY)
        
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small", openai_api_key=OPENAI_API_KEY)
        vectorstore = SupabaseVectorStore(client=supabase_conn.client, embedding=embeddings,
                                          table_name="documents", query_name="match_documents")
        retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
        
        # Enhanced system prompt with better context handling
        system_prompt = """‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶è‡¶ï‡¶ú‡¶® ‡¶∏‡¶π‡¶æ‡¶Ø‡¶º‡¶ï ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶∏‡¶π‡¶ï‡¶æ‡¶∞‡ßÄ ‡¶Ø‡¶ø‡¶®‡¶ø ‡¶ï‡¶•‡ßã‡¶™‡¶ï‡¶•‡¶®‡ßá‡¶∞ ‡¶ß‡¶æ‡¶∞‡¶æ‡¶¨‡¶æ‡¶π‡¶ø‡¶ï‡¶§‡¶æ ‡¶¨‡¶ú‡¶æ‡¶Ø‡¶º ‡¶∞‡¶æ‡¶ñ‡ßá‡¶®‡•§

‡¶®‡¶ø‡¶Ø‡¶º‡¶Æ‡¶æ‡¶¨‡¶≤‡ßÄ:
1. ‡¶®‡¶ø‡¶ö‡ßá‡¶∞ ‡¶§‡¶•‡ßç‡¶Ø‡¶∏‡ßÇ‡¶§‡ßç‡¶∞ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶¶‡¶æ‡¶ì
2. ‡¶Ø‡¶¶‡¶ø ‡¶§‡¶•‡ßç‡¶Ø‡¶∏‡ßÇ‡¶§‡ßç‡¶∞‡ßá ‡¶∏‡¶∞‡¶æ‡¶∏‡¶∞‡¶ø ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶®‡¶æ ‡¶•‡¶æ‡¶ï‡ßá, ‡¶§‡¶¨‡ßá ‡¶¨‡¶≤‡ßã '‡¶§‡¶•‡ßç‡¶Ø ‡¶®‡ßá‡¶á'
3. ‡¶™‡ßÇ‡¶∞‡ßç‡¶¨‡¶¨‡¶∞‡ßç‡¶§‡ßÄ ‡¶ï‡¶•‡ßã‡¶™‡¶ï‡¶•‡¶®‡ßá‡¶∞ ‡¶™‡ßç‡¶∞‡¶∏‡¶ô‡ßç‡¶ó ‡¶Æ‡¶®‡ßá ‡¶∞‡ßá‡¶ñ‡ßã
4. ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞‡¶ï‡¶æ‡¶∞‡ßÄ ‡¶Ø‡¶¶‡¶ø ‡¶™‡ßÇ‡¶∞‡ßç‡¶¨‡ßá‡¶∞ ‡¶™‡ßç‡¶∞‡¶∏‡¶ô‡ßç‡¶ó ‡¶â‡¶≤‡ßç‡¶≤‡ßá‡¶ñ ‡¶ï‡¶∞‡ßá (‡¶Ø‡ßá‡¶Æ‡¶® "‡¶∏‡ßá", "‡¶§‡¶æ‡¶∞", "‡¶è‡¶ü‡¶æ", "‡¶ê ‡¶¨‡ßç‡¶Ø‡¶ï‡ßç‡¶§‡¶ø"), ‡¶§‡¶æ‡¶π‡¶≤‡ßá ‡¶ï‡¶•‡ßã‡¶™‡¶ï‡¶•‡¶®‡ßá‡¶∞ ‡¶á‡¶§‡¶ø‡¶π‡¶æ‡¶∏ ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡¶Ø‡¶º‡ßÄ ‡¶¨‡ßÅ‡¶ù‡ßá ‡¶®‡¶æ‡¶ì
5. ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞‡¶ï‡¶æ‡¶∞‡ßÄ ‡¶Ø‡¶¶‡¶ø ‡¶Ü‡¶ó‡ßá‡¶∞ ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶¨‡¶æ ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶∏‡¶Æ‡ßç‡¶™‡¶∞‡ßç‡¶ï‡ßá ‡¶ú‡¶ø‡¶ú‡ßç‡¶û‡¶æ‡¶∏‡¶æ ‡¶ï‡¶∞‡ßá, ‡¶§‡¶æ‡¶π‡¶≤‡ßá ‡¶ï‡¶•‡ßã‡¶™‡¶ï‡¶•‡¶®‡ßá‡¶∞ ‡¶á‡¶§‡¶ø‡¶π‡¶æ‡¶∏ ‡¶•‡ßá‡¶ï‡ßá ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶¶‡¶æ‡¶ì

‡¶™‡ßÇ‡¶∞‡ßç‡¶¨‡¶¨‡¶∞‡ßç‡¶§‡ßÄ ‡¶ï‡¶•‡ßã‡¶™‡¶ï‡¶•‡¶®:
{conversation_history}

‡¶¨‡¶∞‡ßç‡¶§‡¶Æ‡¶æ‡¶® ‡¶§‡¶•‡ßç‡¶Ø‡¶∏‡ßÇ‡¶§‡ßç‡¶∞:
{context}

‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£: ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞‡¶ï‡¶æ‡¶∞‡ßÄ‡¶∞ ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®‡ßá ‡¶Ø‡¶¶‡¶ø ‡¶Ü‡¶ó‡ßá‡¶∞ ‡¶ï‡¶•‡ßã‡¶™‡¶ï‡¶•‡¶®‡ßá‡¶∞ ‡¶∞‡ßá‡¶´‡¶æ‡¶∞‡ßá‡¶®‡ßç‡¶∏ ‡¶•‡¶æ‡¶ï‡ßá, ‡¶∏‡ßá‡¶ü‡¶æ ‡¶¨‡¶ø‡¶¨‡ßá‡¶ö‡¶®‡¶æ ‡¶ï‡¶∞‡ßá ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶¶‡¶æ‡¶ì‡•§"""
        
        prompt = ChatPromptTemplate.from_messages([("system", system_prompt), ("human", "{input}")])
        llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.3, api_key=OPENAI_API_KEY)
        qa_chain = create_stuff_documents_chain(llm, prompt)
        rag_chain = create_retrieval_chain(retriever, qa_chain)
        
        return rag_chain, None
    except Exception as e:
        return None, str(e)

# Function to get detailed conversation context
def get_conversation_context():
    if len(st.session_state.messages) == 0:
        return ""
    
    # Get last 6 messages for better context (3 exchanges)
    recent_messages = st.session_state.messages[-6:]
    context = ""
    
    for i, msg in enumerate(recent_messages):
        role = "‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞‡¶ï‡¶æ‡¶∞‡ßÄ" if msg["role"] == "user" else "‡¶∏‡¶π‡¶æ‡¶Ø‡¶º‡¶ï"
        context += f"{role}: {msg['content']}\n"
    
    return context

# Function to check if question refers to previous context
def needs_conversation_context(question):
    """Check if the question refers to previous conversation"""
    context_indicators = [
        '‡¶∏‡ßá', '‡¶§‡¶ø‡¶®‡¶ø', '‡¶§‡¶æ‡¶∞', '‡¶ì‡¶∞', '‡¶è‡¶ü‡¶æ', '‡¶ì‡¶ü‡¶æ', '‡¶è‡¶á', '‡¶ê', 
        '‡¶Ü‡¶ó‡ßá‡¶∞', '‡¶™‡ßÇ‡¶∞‡ßç‡¶¨‡ßá‡¶∞', '‡¶Ü‡¶Æ‡¶ø ‡¶ï‡¶ø', '‡¶Ü‡¶Æ‡¶ø ‡¶Ø‡¶æ', '‡¶ï‡¶ø ‡¶ú‡¶ø‡¶ú‡ßç‡¶û‡ßá‡¶∏',
        'valo', '‡¶≠‡¶æ‡¶≤‡ßã', 'she', '‡¶§‡¶æ‡¶π‡¶≤‡ßá', 'why', '‡¶ï‡ßá‡¶®',
        'question', '‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®', 'ami ki', 'what i', 'so far'
    ]
    
    question_lower = question.lower()
    return any(indicator in question_lower for indicator in context_indicators)

# Check connections
(sup_status, sup_msg), (open_status, open_msg) = check_connections()

# Sidebar
with st.sidebar:
    st.markdown("### üîó Connection Status")
    st.write(sup_msg)
    st.write(open_msg)
    
    st.markdown("### üí¨ Chat Controls")
    st.write(f"üìù Messages: {len(st.session_state.messages)}")
    
    # Debug mode toggle
    st.session_state.debug_mode = st.checkbox("üêõ Debug Mode", value=st.session_state.debug_mode)
    
    if st.button("üóëÔ∏è Clear Chat History"):
        st.session_state.messages = []
        st.success("Chat cleared!")
        time.sleep(0.5)
        st.rerun()
    
    # Export chat option
    if len(st.session_state.messages) > 0:
        chat_export = ""
        for msg in st.session_state.messages:
            role = "üë§ ‡¶Ü‡¶™‡¶®‡¶ø" if msg["role"] == "user" else "ü§ñ ‡¶∏‡¶π‡¶æ‡¶Ø‡¶º‡¶ï"
            chat_export += f"{role}: {msg['content']}\n\n"
        
        st.download_button(
            "üíæ Export Chat",
            data=chat_export,
            file_name=f"chat_history_{int(time.time())}.txt",
            mime="text/plain"
        )

# Debug section
if st.session_state.debug_mode:
    with st.sidebar:
        st.markdown("### üîç Debug Info")
        if len(st.session_state.messages) > 0:
            st.write("**Last 3 Messages:**")
            for i, msg in enumerate(st.session_state.messages[-3:]):
                st.text(f"{i+1}. {msg['role']}: {msg['content'][:50]}...")
            
            st.write("**Current Context:**")
            context = get_conversation_context()
            st.text_area("Context", context, height=100, key="debug_context")

# Check if connections are working
if not (sup_status and open_status):
    st.error("‚ùå Connection failed. Please check your configuration.")
    st.stop()

# Setup RAG chain if not already done
if st.session_state.rag_chain is None:
    with st.spinner("Setting up RAG system..."):
        rag_chain, error = setup_rag_chain()
        if rag_chain:
            st.session_state.rag_chain = rag_chain
            st.success("‚úÖ RAG system ready!")
        else:
            st.error(f"‚ùå RAG setup failed: {error}")
            st.stop()

# Display chat messages
st.markdown("### üí¨ ‡¶ï‡¶•‡ßã‡¶™‡¶ï‡¶•‡¶®")

# Create a container for chat messages
chat_container = st.container()

with chat_container:
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            # Show sources for assistant messages
            if message["role"] == "assistant" and "sources" in message:
                with st.expander("üìö ‡¶§‡¶•‡ßç‡¶Ø‡¶∏‡ßÇ‡¶§‡ßç‡¶∞"):
                    for i, source in enumerate(message["sources"], 1):
                        st.markdown(f"**Source {i}:** {source}")

# Chat input
if prompt := st.chat_input("‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶≤‡¶ø‡¶ñ‡ßÅ‡¶®..."):
    # Add user message to chat history
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # Display user message
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # Generate assistant response
    with st.chat_message("assistant"):
        with st.spinner("‡¶ö‡¶ø‡¶®‡ßç‡¶§‡¶æ ‡¶ï‡¶∞‡¶õ‡¶ø..."):
            try:
                # Get conversation context
                conversation_context = get_conversation_context()
                
                # Check if question needs conversation context
                context_needed = needs_conversation_context(prompt)
                
                # Debug info
                if st.session_state.debug_mode:
                    st.write("üîç **Debug Info:**")
                    st.write(f"- Context needed: {context_needed}")
                    st.write(f"- Conversation length: {len(st.session_state.messages)}")
                    if conversation_context:
                        st.write(f"- Context preview: {conversation_context[:100]}...")
                
                # Create the input for RAG
                if context_needed and conversation_context:
                    # For context-dependent questions, add more conversation history
                    rag_input = {
                        "input": prompt,
                        "conversation_history": conversation_context
                    }
                else:
                    # For independent questions, minimal context
                    rag_input = {
                        "input": prompt,
                        "conversation_history": conversation_context[-200:] if conversation_context else ""
                    }
                
                # Get response from RAG
                result = st.session_state.rag_chain.invoke(rag_input)
                response = result.get("answer", "‡¶§‡¶•‡ßç‡¶Ø ‡¶®‡ßá‡¶á")
                
                # Extract sources
                sources = []
                if "context" in result and result["context"]:
                    sources = [doc.page_content[:200] + "..." for doc in result["context"]]
                
                # If asking about previous questions and no good answer, try to answer from chat history
                if (response == "‡¶§‡¶•‡ßç‡¶Ø ‡¶®‡ßá‡¶á" and context_needed and 
                    any(word in prompt.lower() for word in ['ami ki', 'what i', 'question', '‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®', '‡¶ú‡¶ø‡¶ú‡ßç‡¶û‡ßá‡¶∏'])):
                    
                    # Extract questions from chat history
                    user_questions = [msg["content"] for msg in st.session_state.messages if msg["role"] == "user"]
                    if user_questions:
                        if "so far" in prompt.lower() or "‡¶ï‡¶ø ‡¶ï‡¶ø" in prompt:
                            response = f"‡¶Ü‡¶™‡¶®‡¶ø ‡¶è ‡¶™‡¶∞‡ßç‡¶Ø‡¶®‡ßç‡¶§ ‡¶Ø‡ßá ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®‡¶ó‡ßÅ‡¶≤‡ßã ‡¶ï‡¶∞‡ßá‡¶õ‡ßá‡¶®:\n\n"
                            for i, q in enumerate(user_questions[:-1], 1):  # Exclude current question
                                response += f"{i}. {q}\n"
                        else:
                            response = f"‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶∂‡ßá‡¶∑ ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶õ‡¶ø‡¶≤: '{user_questions[-2] if len(user_questions) > 1 else '‡¶ï‡ßã‡¶®‡ßã ‡¶Ü‡¶ó‡ßá‡¶∞ ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶®‡ßá‡¶á'}'"
                
                # Display response
                st.markdown(response)
                
                # Debug: Show what was sent to RAG
                if st.session_state.debug_mode:
                    st.write("üîß **RAG Input:**")
                    st.json(rag_input)
                
                # Add assistant response to chat history
                st.session_state.messages.append({
                    "role": "assistant", 
                    "content": response,
                    "sources": sources
                })
                
            except Exception as e:
                error_msg = f"‡¶¶‡ßÅ‡¶É‡¶ñ‡¶ø‡¶§, ‡¶è‡¶ï‡¶ü‡¶ø ‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá: {str(e)}"
                st.error(error_msg)
                st.session_state.messages.append({
                    "role": "assistant", 
                    "content": error_msg
                })

# Quick action buttons
if len(st.session_state.messages) == 0:
    st.markdown("### üöÄ ‡¶¶‡ßç‡¶∞‡ßÅ‡¶§ ‡¶∂‡ßÅ‡¶∞‡ßÅ ‡¶ï‡¶∞‡ßÅ‡¶®")
    st.markdown("‡¶®‡¶ø‡¶ö‡ßá‡¶∞ ‡¶¨‡¶ø‡¶∑‡¶Ø‡¶º‡¶ó‡ßÅ‡¶≤‡ßã ‡¶∏‡¶Æ‡ßç‡¶™‡¶∞‡ßç‡¶ï‡ßá ‡¶ú‡¶ø‡¶ú‡ßç‡¶û‡¶æ‡¶∏‡¶æ ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡¶®:")
    
    col1, col2, col3, col4 = st.columns(4)
    
    quick_questions = [
        ("üìñ", "‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶∏‡¶æ‡¶π‡¶ø‡¶§‡ßç‡¶Ø", "‡¶ï‡¶≤‡ßç‡¶Ø‡¶æ‡¶£‡ßÄ‡¶∞ ‡¶™‡¶ø‡¶§‡¶æ‡¶∞ ‡¶®‡¶æ‡¶Æ ‡¶ï‡ßÄ?")
    ]
    
    cols = [col1, col2, col3, col4]
    for i, (emoji, title, question) in enumerate(quick_questions):
        with cols[i]:
            if st.button(f"{emoji} {title}"):
                # Add the question as if user typed it
                st.session_state.messages.append({"role": "user", "content": question})
                st.rerun()

# Tips section
if len(st.session_state.messages) > 0:
    st.markdown("""
<div style='
    background-color: #23272f;
    padding: 14px 16px;
    border-radius: 8px;
    margin-top: 12px;
    color: #f3f6fb;
    font-size: 15px;
    border-left: 4px solid #3b82f6;'
>
    <small>
    üí° <strong>‡¶ü‡¶ø‡¶™‡¶∏:</strong><br>
    ‚Ä¢ "‡¶∏‡ßá ‡¶ï‡ßá‡¶Æ‡¶®?" - ‡¶Ü‡¶ó‡ßá‡¶∞ ‡¶ï‡¶•‡ßã‡¶™‡¶ï‡¶•‡¶®‡ßá‡¶∞ ‡¶¨‡ßç‡¶Ø‡¶ï‡ßç‡¶§‡¶ø ‡¶∏‡¶Æ‡ßç‡¶™‡¶∞‡ßç‡¶ï‡ßá ‡¶ú‡¶ø‡¶ú‡ßç‡¶û‡ßá‡¶∏ ‡¶ï‡¶∞‡ßÅ‡¶®<br>
    ‚Ä¢ "‡¶Ü‡¶Æ‡¶ø ‡¶ï‡¶ø ‡¶ï‡¶ø ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶ï‡¶∞‡ßá‡¶õ‡¶ø?" - ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®‡ßá‡¶∞ ‡¶§‡¶æ‡¶≤‡¶ø‡¶ï‡¶æ ‡¶¶‡ßá‡¶ñ‡ßÅ‡¶®<br>
    ‚Ä¢ "‡¶è‡¶ü‡¶æ ‡¶ï‡ßÄ?" - ‡¶Ü‡¶ó‡ßá‡¶∞ ‡¶â‡¶≤‡ßç‡¶≤‡ßá‡¶ñ‡¶ø‡¶§ ‡¶¨‡¶ø‡¶∑‡¶Ø‡¶º ‡¶∏‡¶Æ‡ßç‡¶™‡¶∞‡ßç‡¶ï‡ßá ‡¶Ü‡¶∞‡¶ì ‡¶ú‡¶æ‡¶®‡ßÅ‡¶®
    </small>
</div>
""", unsafe_allow_html=True)


# Footer
st.markdown("---")
st.markdown("""
<div style='text-align: center; color: #666; font-size: 12px;'>
    üöÄ Powered by LangChain, Supabase, OpenAI, and Streamlit<br>
    üß† Enhanced with Conversation Memory & Context Awareness
</div>
""", unsafe_allow_html=True)
